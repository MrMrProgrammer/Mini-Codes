{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests as req\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from urllib3 import disable_warnings\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rssfinder_function(url):\n",
    "\n",
    "    print(\" *** RSS Finder Function Started ! *** \")\n",
    "\n",
    "    global used_url\n",
    "    global finded_rss\n",
    "\n",
    "    used_url = []\n",
    "    finded_rss = []\n",
    "\n",
    "    def Standard_Domain(url):\n",
    "\n",
    "        try :\n",
    "\n",
    "            # ***********************************************************\n",
    "            # ***********************************************************\n",
    "\n",
    "            def is_rss(url):\n",
    "\n",
    "                print(\"'\" + url + \"' is Checking ...\" )\n",
    "\n",
    "                try :\n",
    "                    feed = feedparser.parse(url)\n",
    "                    feed_entries = feed.entries\n",
    "\n",
    "                    if len(feed_entries) != 0:\n",
    "                        return True\n",
    "                    \n",
    "                    else:\n",
    "                        if req.get(url ).status_code == 200 :\n",
    "                            res = req.get(url)\n",
    "                            content_type = res.headers[ \"Content-Type\" ]\n",
    "                            \n",
    "                            if \"application/rss+xml\" in content_type :\n",
    "                                return True\n",
    "\n",
    "                            elif \"application/xml\" in content_type :\n",
    "                                return True\n",
    "\n",
    "                            elif \"text/html\" in content_type or \"text/xml;\" in content_type:\n",
    "                                res = req.get(url)\n",
    "                                soup = bs(res.text , \"html.parser\")\n",
    "\n",
    "                                all_tags = soup.find_all()\n",
    "\n",
    "                                if all_tags != None :\n",
    "                                    for i in all_tags :\n",
    "                                        if i.name == \"rss\" or i.name == \"feed\" or i.name == \"feeds\" or i.name == \"channel\" :\n",
    "                                            return True\n",
    "\n",
    "                        elif req.get(url ).status_code == 403 :\n",
    "                            res = req.get(url)\n",
    "                            content_type = res.headers[ \"Content-Type\" ]\n",
    "                            \n",
    "                            if \"application/rss+xml\" in content_type :\n",
    "                                return True\n",
    "\n",
    "                            elif \"application/xml\" in content_type :\n",
    "                                return True\n",
    "\n",
    "                            elif \"text/html\" in content_type or \"text/xml;\" in content_type:\n",
    "                                res = req.get(url)\n",
    "                                soup = bs(res.text , \"html.parser\")\n",
    "\n",
    "                                all_tags = soup.find_all()\n",
    "\n",
    "                                if all_tags != None :\n",
    "                                    for i in all_tags :\n",
    "                                        if i.name == \"rss\" or i.name == \"feed\" or i.name == \"feeds\" or i.name == \"channel\" :\n",
    "                                            return True\n",
    "                                            \n",
    "                        else :\n",
    "                            return False\n",
    "                \n",
    "                except :\n",
    "                    return False\n",
    "\n",
    "            # ***********************************************************\n",
    "            # ***********************************************************\n",
    "\n",
    "            def find_all_rss(url):\n",
    "                if url[0] == \"/\" :\n",
    "                    url = org_url + url\n",
    "\n",
    "                try :\n",
    "\n",
    "                    rss_link = []\n",
    "\n",
    "                    res = req.get(url , verify=False)\n",
    "                    soup = bs(res.text , \"html.parser\")\n",
    "\n",
    "                    all_links = soup.find_all(\"a\")\n",
    "\n",
    "                    for i in all_links :\n",
    "                        try :\n",
    "                            if \"rss\" in i.get(\"href\") or \"RSS\" in i.get(\"href\") or \"Rss\" in i.get(\"href\") or \"feed\" in i.get(\"href\") or \"FEED\" in i.get(\"href\") or \"Feed\" in i.get(\"href\") :\n",
    "                                rss_link.append(i.get(\"href\"))\n",
    "\n",
    "                        except :\n",
    "                            continue\n",
    "\n",
    "                    return rss_link\n",
    "\n",
    "                except :\n",
    "                    pass\n",
    "\n",
    "            # ***********************************************************\n",
    "            # ***********************************************************\n",
    "\n",
    "            def main(url):\n",
    "\n",
    "                used_url.append(url)\n",
    "\n",
    "                if url[0] == \".\" :\n",
    "                    url = url[1:]\n",
    "                \n",
    "                if url[0] == \"/\" :\n",
    "                    url = org_url + url\n",
    "\n",
    "                used_url.append(url)\n",
    "\n",
    "                # print(url)\n",
    "\n",
    "                # status = is_url_valid(url)\n",
    "\n",
    "                try :\n",
    "\n",
    "                    status = req.get(url).status_code\n",
    "\n",
    "                    if status == 200 :\n",
    "\n",
    "                        rss = is_rss(url)\n",
    "\n",
    "                        if rss == True :\n",
    "\n",
    "                            finded_rss.append(url)\n",
    "                            # print( f\"{url} is RSS\" )\n",
    "\n",
    "                        else :\n",
    "\n",
    "                            all_links = find_all_rss(url)\n",
    "\n",
    "                            if all_links != None :\n",
    "                                for i in all_links :\n",
    "                                    if i not in used_url :\n",
    "                                        main(i)\n",
    "                                    \n",
    "                                    else :\n",
    "                                        continue\n",
    "\n",
    "                except :\n",
    "                    pass\n",
    "\n",
    "            # ***********************************************************\n",
    "            # ***********************************************************\n",
    "\n",
    "            def beta(url):\n",
    "\n",
    "                try :\n",
    "\n",
    "                    res = req.get(url)\n",
    "                    soup = bs(res.text , \"html.parser\")\n",
    "\n",
    "                    all_link = soup.find_all(\"a\")\n",
    "\n",
    "                    if len(all_link) != 0 :\n",
    "\n",
    "                        for i in all_link:\n",
    "\n",
    "                            try :\n",
    "\n",
    "                                main(i.get(\"href\"))\n",
    "                            \n",
    "                            except :\n",
    "                                continue\n",
    "\n",
    "                except :\n",
    "                    pass\n",
    "\n",
    "            # ***********************************************************\n",
    "            # ***********************************************************\n",
    "\n",
    "            def gama(url) :\n",
    "\n",
    "                org = url\n",
    "\n",
    "                dictionary = [ \"feed\" , \"FEED\" , \"Feed\" , \"RSS\" , \"Rss\" , \"rss\" , \"fa/rss\" , \"fa/RSS\" , \"feeds\" , \"FEEDS\" , \"Feeds\" , \"FEEDs\" , \"RssForNews\" ]\n",
    "\n",
    "                for i in dictionary :\n",
    "\n",
    "                    try:\n",
    "                        pat_url = org + \"/\" + i\n",
    "                        main(pat_url)\n",
    "\n",
    "                    except :\n",
    "                        continue\n",
    "\n",
    "\n",
    "                if len(finded_rss) == 0:\n",
    "                    if \"www.\" in url or \"WWW.\" in url or \"Www.\" in url or \"WWw.\" in url or \"wWW.\" in url or \"wwW.\" in url :\n",
    "                        if \"https://\" in org and \"http://\" not in org:\n",
    "                            for i in dictionary :\n",
    "                                try :\n",
    "                                    pat_url = url.replace( \"https://www.\" , f\"https://www.{i}.\" )\n",
    "\n",
    "                                    if is_rss(pat_url) == True :\n",
    "                                        finded_rss.append(pat_url)\n",
    "                                \n",
    "                                except :\n",
    "                                    continue\n",
    "\n",
    "                        elif \"http://\" in org and \"https://\" not in org :\n",
    "                            for i in dictionary :\n",
    "\n",
    "                                try :\n",
    "                                    pat_url = url.replace( \"http://www.\" , f\"http://www.{i}.\" )\n",
    "\n",
    "                                    if is_rss(pat_url) == True :\n",
    "                                        finded_rss.append(pat_url)\n",
    "\n",
    "                                except :\n",
    "                                    continue\n",
    "\n",
    "                    else :\n",
    "                        if \"https://\" in org and \"http://\" not in org:\n",
    "                            for i in dictionary :\n",
    "                                try :\n",
    "                                    pat_url = url.replace( \"https://\" , f\"https://{i}.\" )\n",
    "\n",
    "                                    if is_rss(pat_url) == True :\n",
    "                                        finded_rss.append(pat_url)\n",
    "                                \n",
    "                                except :\n",
    "                                    continue\n",
    "\n",
    "                        elif \"http://\" in org and \"https://\" not in org :\n",
    "                            for i in dictionary :\n",
    "\n",
    "                                try :\n",
    "                                    pat_url = url.replace( \"http://\" , f\"http://{i}.{url}\" )\n",
    "\n",
    "                                    if is_rss(pat_url) == True :\n",
    "                                        finded_rss.append(pat_url)\n",
    "\n",
    "                                except :\n",
    "                                    continue\n",
    "\n",
    "\n",
    "            # ***********************************************************\n",
    "            # ***********************************************************\n",
    "\n",
    "            org_url = url\n",
    "            main(url)\n",
    "\n",
    "            if len(finded_rss) == 0 :\n",
    "                beta(org_url)\n",
    "\n",
    "            if len(finded_rss) == 0 :\n",
    "                gama(org_url)\n",
    "\n",
    "        except :\n",
    "            pass\n",
    "\n",
    "# =========================================================================================\n",
    "# =========================================================================================\n",
    "\n",
    "    def unStandard_Domain(url):\n",
    "\n",
    "        try :\n",
    "            # ***********************************************************\n",
    "            # ***********************************************************\n",
    "\n",
    "            def use_selenium (url):\n",
    "\n",
    "                try :\n",
    "\n",
    "                    # print(\" 'use_selenium' function() \")\n",
    "\n",
    "                    options = webdriver.ChromeOptions()\n",
    "                    options.add_argument('headless')\n",
    "                    driver = webdriver.Chrome(ChromeDriverManager().install() , chrome_options=options)\n",
    "                    driver.get(url)\n",
    "                    html = driver.page_source\n",
    "                    soup = bs(html)\n",
    "\n",
    "                    return soup\n",
    "                \n",
    "                except :\n",
    "                    pass\n",
    "\n",
    "            # ***********************************************************\n",
    "            # ***********************************************************\n",
    "\n",
    "            def is_rss_selenium(url):\n",
    "\n",
    "                print(\"'\" + url + \"' is Checking ...\" )\n",
    "\n",
    "                # print(\" 'is_rss_selenium' function() \")\n",
    "\n",
    "                try :\n",
    "                    feed = feedparser.parse(url)\n",
    "                    feed_entries = feed.entries\n",
    "\n",
    "                    if len(feed_entries) != 0:\n",
    "                        return True\n",
    "\n",
    "                    else :\n",
    "\n",
    "                        soup = use_selenium(url)\n",
    "\n",
    "                        all_tags = soup.find_all()\n",
    "\n",
    "                        if all_tags != None :\n",
    "                            for i in all_tags :\n",
    "                                if i.name == \"rss\" or i.name == \"feed\" or i.name == \"feeds\" or i.name == \"channel\" :\n",
    "                                    return True\n",
    "                \n",
    "                except :\n",
    "                    return False\n",
    "\n",
    "            # ***********************************************************\n",
    "            # ***********************************************************\n",
    "\n",
    "            def find_all_rss_selenium(url):\n",
    "\n",
    "                # print(\" 'find_all_rss_selenium' function() \")\n",
    "\n",
    "                if url[0] == \"/\" :\n",
    "                    url = org_url + url\n",
    "\n",
    "                try :\n",
    "\n",
    "                    rss_link = []\n",
    "                    soup = use_selenium(url)\n",
    "                    all_links = soup.find_all(\"a\")\n",
    "\n",
    "                    for i in all_links :\n",
    "                        try :\n",
    "                            if \"rss\" in i.get(\"href\") or \"RSS\" in i.get(\"href\") or \"Rss\" in i.get(\"href\") or \"feed\" in i.get(\"href\") or \"FEED\" in i.get(\"href\") or \"Feed\" in i.get(\"href\") :\n",
    "                                rss_link.append(i.get(\"href\"))\n",
    "\n",
    "                        except :\n",
    "                            continue\n",
    "\n",
    "                    return rss_link\n",
    "\n",
    "                except :\n",
    "                    pass\n",
    "\n",
    "            # ***********************************************************\n",
    "            # ***********************************************************\n",
    "\n",
    "            def main_selenium(url):\n",
    "\n",
    "                try :\n",
    "\n",
    "                    # print(\" 'main_selenium' function() \")\n",
    "\n",
    "                    used_url.append(url)\n",
    "\n",
    "                    if url[0] == \".\" :\n",
    "                        url = url[1:]\n",
    "                    \n",
    "                    if url[0] == \"/\" :\n",
    "                        url = org_url + url\n",
    "\n",
    "                    used_url.append(url)\n",
    "\n",
    "                    # print(url)\n",
    "\n",
    "                    rss = is_rss_selenium(url)\n",
    "\n",
    "                    if rss == True :\n",
    "\n",
    "                        finded_rss.append(url)\n",
    "                        # print( f\"{url} is RSS\" )\n",
    "\n",
    "                    else :\n",
    "\n",
    "                        all_links = find_all_rss_selenium(url)\n",
    "\n",
    "                        if all_links != None :\n",
    "                            for i in all_links :\n",
    "                                try :\n",
    "                                    if i not in used_url :\n",
    "                                        main_selenium(i)\n",
    "                                    \n",
    "                                    else :\n",
    "                                        continue\n",
    "\n",
    "                                except:\n",
    "                                    continue\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # ***********************************************************\n",
    "            # ***********************************************************\n",
    "\n",
    "            def beta_selenium(url):\n",
    "\n",
    "                try :\n",
    "\n",
    "                    soup = use_selenium(url)\n",
    "\n",
    "                    all_link = soup.find_all(\"a\")\n",
    "\n",
    "                    if len(all_link) != 0 :\n",
    "\n",
    "                        for i in all_link:\n",
    "\n",
    "                            try:\n",
    "                                main_selenium(i.get(\"href\"))\n",
    "                            except:\n",
    "                                continue\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # ***********************************************************\n",
    "            # ***********************************************************\n",
    "\n",
    "            def gama_selenium(url) :\n",
    "\n",
    "                org = url\n",
    "\n",
    "                dictionary = [ \"feed\" , \"FEED\" , \"Feed\" , \"RSS\" , \"Rss\" , \"rss\" , \"fa/rss\" , \"fa/RSS\" , \"feeds\" , \"FEEDS\" , \"Feeds\" , \"FEEDs\" , \"RrrForNews\" ]\n",
    "\n",
    "                for i in dictionary :\n",
    "\n",
    "                    try:      \n",
    "                        pat_url = org + \"/\" + i\n",
    "                        main_selenium(pat_url)\n",
    "\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                if len(finded_rss) == 0:\n",
    "\n",
    "                    if \"www.\" in url or \"WWW.\" in url or \"Www.\" in url or \"WWw.\" in url or \"wWW.\" in url or \"wwW.\" in url :\n",
    "                        if \"https://\" in org and \"http://\" not in org:\n",
    "                            for i in dictionary :\n",
    "                                try:\n",
    "                                    pat_url = url.replace( \"https://www.\" , f\"https://www.{i}.\" )\n",
    "                                    if is_rss_selenium(pat_url) == True :\n",
    "                                        finded_rss.append(pat_url)\n",
    "                                except:\n",
    "                                    continue\n",
    "\n",
    "                        elif \"http://\" in org and \"https://\" not in org :\n",
    "                            for i in dictionary :\n",
    "                                try :  \n",
    "                                    pat_url = url.replace( \"http://\" , f\"http://www.{i}.\" )\n",
    "                                    if is_rss_selenium(pat_url) == True :\n",
    "                                        finded_rss.append(pat_url)\n",
    "                                except:\n",
    "                                    continue\n",
    "\n",
    "                    else :\n",
    "                        if \"https://\" in org and \"http://\" not in org:\n",
    "                            for i in dictionary :\n",
    "                                try:\n",
    "                                    pat_url = url.replace( \"https://\" , f\"https://{i}.\" )\n",
    "                                    if is_rss_selenium(pat_url) == True :\n",
    "                                        finded_rss.append(pat_url)\n",
    "                                except:\n",
    "                                    continue\n",
    "\n",
    "                        elif \"http://\" in org and \"https://\" not in org :\n",
    "                            for i in dictionary :\n",
    "                                try :  \n",
    "                                    pat_url = url.replace( \"http://\" , f\"http://{i}.{url}\" )\n",
    "                                    if is_rss_selenium(pat_url) == True :\n",
    "                                        finded_rss.append(pat_url)\n",
    "                                except:\n",
    "                                    continue\n",
    "\n",
    "\n",
    "            # ***********************************************************\n",
    "            # ***********************************************************\n",
    "            \n",
    "            org_url = url\n",
    "            main_selenium(url)\n",
    "            \n",
    "\n",
    "            print(\"befor beta\")\n",
    "            print(\"len findedrss : \" , len(finded_rss))\n",
    "\n",
    "            if len(finded_rss) == 0 :\n",
    "                beta_selenium(org_url)\n",
    "\n",
    "\n",
    "            print(\"befor gama\")\n",
    "            print(\"len findedrss : \" , len(finded_rss))\n",
    "\n",
    "            if len(finded_rss) == 0 :\n",
    "                gama_selenium(org_url)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def pre_set (url) :\n",
    "\n",
    "        urls = []\n",
    "\n",
    "        if \"https\" in url :\n",
    "            urls.append(url)\n",
    "            url = url.replace( \"https\" , \"http\" )\n",
    "            urls.append(url)\n",
    "\n",
    "        elif \"https\" not in url and \"http\" in url :\n",
    "\n",
    "            url = url.replace( \"http\" , \"https\" )\n",
    "            urls.append(url)\n",
    "            url = url.replace( \"https\" , \"http\" )\n",
    "            urls.append(url)\n",
    "\n",
    "        elif \"https\" not in url and \"http\" not in url :\n",
    "\n",
    "            url_01 = \"https://\" + url\n",
    "            urls.append(url_01)\n",
    "\n",
    "            url_02 = \"http://\" + url\n",
    "            urls.append(url_02)\n",
    "\n",
    "        return urls\n",
    "\n",
    "# =========================================================================================\n",
    "# =========================================================================================\n",
    "\n",
    "    urls = pre_set(url)\n",
    "\n",
    "    # 00\n",
    "    try :\n",
    "\n",
    "        status = req.get(urls[0]).status_code\n",
    "\n",
    "        if status == 200 :\n",
    "\n",
    "            Standard_Domain(urls[0])\n",
    "\n",
    "            if len(finded_rss) == 0 :\n",
    "\n",
    "                Standard_Domain(urls[1])\n",
    "\n",
    "        \n",
    "        elif status == 403 or status == 500 :\n",
    "\n",
    "            unStandard_Domain(urls[0])\n",
    "\n",
    "            if len(finded_rss) == 0 :\n",
    "\n",
    "                unStandard_Domain(urls[1])\n",
    "\n",
    "\n",
    "    except :\n",
    "\n",
    "        unStandard_Domain(urls[0])\n",
    "\n",
    "        if len(finded_rss) == 0 :\n",
    "\n",
    "            unStandard_Domain(urls[1])\n",
    "\n",
    "    \n",
    "    print(\" *** RSS Finder Function Finished ! *** \")\n",
    "    return finded_rss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rssfinder_function(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
